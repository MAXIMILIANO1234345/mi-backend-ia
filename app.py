import os
import json
import re
import google.generativeai as genai
from flask import Flask, request, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
from supabase import create_client, Client

# --- 1. CONFIGURACI√ìN ---
print("--- Iniciando CEREBRO ORQUESTADOR (Modo Estrella) ---")
load_dotenv()

GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_KEY')

if not all([GOOGLE_API_KEY, SUPABASE_URL, SUPABASE_KEY]):
    raise ValueError("‚ö†Ô∏è Faltan variables de entorno (.env)")

# Configuraci√≥n de Clientes
genai.configure(api_key=GOOGLE_API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# CONSTANTES DEL ORQUESTADOR
ORQUESTADOR_ID = 1 # Asumimos que somos el "Alpha" (ID 1)
EMBEDDING_MODEL = "models/text-embedding-004"
GENERATIVE_MODEL = "models/gemini-2.5-flash" 

app = Flask(__name__)
CORS(app)

# --- 2. CACHE DE PILARES (Memoria de Trabajo) ---
# Al iniciar, cargamos el mapa del c√≠rculo para no consultar SQL a cada rato
CATALOGO_PILARES = {} 

def cargar_catalogo():
    """Descarga el mapa mental del Orquestador desde Supabase."""
    global CATALOGO_PILARES
    try:
        # Traemos solo los pilares de ESTE orquestador
        response = supabase.table('catalogo_pilares')\
            .select('nombre_clave, nombre_tabla, descripcion')\
            .eq('orquestador_id', ORQUESTADOR_ID)\
            .execute()
        
        if response.data:
            CATALOGO_PILARES = {item['nombre_clave']: item for item in response.data}
            print(f"‚úÖ Cat√°logo cargado: {len(CATALOGO_PILARES)} pilares disponibles.")
        else:
            print("‚ö†Ô∏è ADVERTENCIA: El cat√°logo est√° vac√≠o. Ejecuta el SQL de setup.")
    except Exception as e:
        print(f"‚ùå Error cargando cat√°logo: {e}")

# Cargar al inicio
cargar_catalogo()

# --- 3. UTILIDADES ---

def limpiar_json(texto):
    """Limpia respuestas del LLM para obtener JSON puro."""
    texto = texto.strip()
    # Eliminar bloques de c√≥digo markdown
    texto = re.sub(r'^```json\s*', '', texto)
    texto = re.sub(r'^```\s*', '', texto)
    texto = re.sub(r'\s*```$', '', texto)
    return texto.strip()

def get_embedding(text):
    """Vectorizaci√≥n est√°ndar."""
    try:
        res = genai.embed_content(model=EMBEDDING_MODEL, content=text, task_type="RETRIEVAL_QUERY")
        return res['embedding']
    except Exception as e:
        print(f"‚ùå Error vectorizando: {e}")
        return None

# --- 4. L√ìGICA DEL ORQUESTADOR (EL CEREBRO) ---

def planificar_busqueda(pregunta):
    """
    Paso 1: El Orquestador decide D√ìNDE buscar.
    No buscamos en todas las tablas, solo en las relevantes.
    """
    modelo = genai.GenerativeModel(GENERATIVE_MODEL)
    
    # Crear lista legible para el prompt
    lista_pilares = "\n".join([f"- {k}: {v['descripcion']}" for k, v in CATALOGO_PILARES.items()])
    
    prompt = f"""
    Eres el Orquestador de una Base de Datos de Conocimiento de Blender.
    
    PREGUNTA DEL USUARIO: "{pregunta}"
    
    TU MEMORIA EST√Å DIVIDIDA EN ESTAS B√ìVEDAS (TABLAS):
    {lista_pilares}
    
    TAREA:
    Identifica en qu√© b√≥veda(s) (1 o 2 m√°ximo) es m√°s probable encontrar la respuesta.
    Si la pregunta es muy general, elige 'logica_ia' o 'api'.
    
    RESPONDE SOLO JSON:
    {{ "pilares_seleccionados": ["nombre_clave_1", "nombre_clave_2"] }}
    """
    
    try:
        res = modelo.generate_content(prompt, generation_config={"response_mime_type": "application/json"})
        data = json.loads(limpiar_json(res.text))
        return data.get("pilares_seleccionados", [])
    except:
        return ["api"] # Fallback seguro

def consultar_memoria(pilares_objetivo, vector_pregunta):
    """
    Paso 2: Ejecuta 'cerebro_recordar' en las tablas seleccionadas.
    """
    hallazgos = []
    
    for clave in pilares_objetivo:
        if clave not in CATALOGO_PILARES: continue
        
        tabla_real = CATALOGO_PILARES[clave]['nombre_tabla']
        print(f"üß† Consultando memoria: {tabla_real}...")
        
        try:
            # Llamada a la RPC centralizada del SQL
            response = supabase.rpc('cerebro_recordar', {
                'p_orquestador_id': ORQUESTADOR_ID,
                'p_tabla_destino': tabla_real,
                'p_vector': vector_pregunta,
                'p_umbral': 0.4, # Umbral de similitud
                'p_limite': 3
            }).execute()
            
            if response.data:
                for item in response.data:
                    hallazgos.append(f"[{clave.upper()}] Concepto: {item['concepto']}\nDetalle: {item['detalle']}\nSimilitud: {item['similitud']:.2f}")
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Error leyendo {tabla_real}: {e}")
            
    return hallazgos

def aprender_y_guardar(pregunta):
    """
    Paso 3 (CR√çTICO): Si no sabemos la respuesta, INVESTIGAMOS y APRENDEMOS.
    """
    print("üåê Modo Aprendizaje Activado: Buscando informaci√≥n externa...")
    modelo = genai.GenerativeModel(GENERATIVE_MODEL)
    
    # 1. INVESTIGAR (Usamos Gemini con herramienta de b√∫squeda si disponible, o simulaci√≥n)
    # Prompt dise√±ado para extraer informaci√≥n estructurada de su conocimiento base + b√∫squeda
    prompt_investigacion = f"""
    El usuario pregunta: "{pregunta}".
    No tengo esta informaci√≥n en mi base de datos local.
    
    Por favor, responde a la pregunta con tu mejor conocimiento experto en Blender y Python.
    S√© t√©cnico, preciso y da ejemplos de c√≥digo si aplica.
    """
    
    res_investigacion = modelo.generate_content(prompt_investigacion)
    info_nueva = res_investigacion.text
    
    # 2. CLASIFICAR Y ESTRUCTURAR (ETL)
    # Ahora que tenemos la info, el Orquestador debe decidir d√≥nde guardarla.
    lista_pilares = "\n".join([f"- {k}: {v['descripcion']}" for k, v in CATALOGO_PILARES.items()])
    
    prompt_clasificacion = f"""
    ANALIZA ESTA INFORMACI√ìN NUEVA:
    "{info_nueva}"
    
    TU CAT√ÅLOGO DE MEMORIA:
    {lista_pilares}
    
    TAREA:
    1. Resume el concepto clave.
    2. Extrae el detalle t√©cnico/c√≥digo.
    3. Decide en QU√â tabla (nombre_clave) debe guardarse.
    
    JSON OBLIGATORIO:
    {{
        "tabla_destino": "nombre_clave_del_catalogo",
        "concepto": "T√≠tulo corto",
        "detalle_tecnico": "Explicaci√≥n t√©cnica resumida",
        "codigo_ejemplo": "snippet de codigo o null"
    }}
    """
    
    try:
        res_clasif = modelo.generate_content(prompt_clasificacion, generation_config={"response_mime_type": "application/json"})
        datos_aprendizaje = json.loads(limpiar_json(res_clasif.text))
        
        clave_destino = datos_aprendizaje.get("tabla_destino")
        
        if clave_destino in CATALOGO_PILARES:
            tabla_real = CATALOGO_PILARES[clave_destino]['nombre_tabla']
            
            # 3. GUARDAR (RPC cerebro_aprender)
            vec_nuevo = get_embedding(f"{datos_aprendizaje['concepto']} {datos_aprendizaje['detalle_tecnico']}")
            
            supabase.rpc('cerebro_aprender', {
                'p_orquestador_id': ORQUESTADOR_ID,
                'p_tabla_destino': tabla_real,
                'p_concepto': datos_aprendizaje['concepto'],
                'p_detalle': datos_aprendizaje['detalle_tecnico'],
                'p_codigo': datos_aprendizaje.get('codigo_ejemplo', ''),
                'p_vector': vec_nuevo
            }).execute()
            
            print(f"üíæ CONOCIMIENTO GUARDADO en {tabla_real}: {datos_aprendizaje['concepto']}")
            return info_nueva + "\n\n(Nota: Acabo de aprender esto y lo he guardado en mi memoria de " + clave_destino + ")."
            
        else:
            return info_nueva + "\n(Nota: No supe d√≥nde clasificar esto, pero aqu√≠ tienes la respuesta)."
            
    except Exception as e:
        print(f"‚ùå Error en aprendizaje: {e}")
        return info_nueva # Devolvemos la info aunque falle el guardado

# --- 5. ENDPOINTS ---

@app.route("/", methods=["GET"])
def health():
    return jsonify({"status": "online", "mode": "Orquestador Estrella Centralizada"}), 200

@app.route("/preguntar", methods=["POST"])
def endpoint_preguntar():
    data = request.json
    pregunta = data.get('pregunta', '')
    
    if not pregunta: return jsonify({"error": "Pregunta vac√≠a"}), 400
    
    print(f"\nüì® Nueva solicitud: '{pregunta}'")
    
    # 1. PLANIFICACI√ìN
    pilares_target = planificar_busqueda(pregunta)
    print(f"üéØ Estrategia: Buscar en {pilares_target}")
    
    # 2. EJECUCI√ìN (B√∫squeda interna)
    vector = get_embedding(pregunta)
    contexto = consultar_memoria(pilares_target, vector)
    
    # 3. EVALUACI√ìN
    if contexto:
        print(f"‚úÖ Encontrado en memoria interna ({len(contexto)} registros).")
        # Generar respuesta con RAG
        modelo = genai.GenerativeModel(GENERATIVE_MODEL)
        prompt = f"""
        CONTEXTO DE TU MEMORIA (BASED ON SQL):
        {chr(10).join(contexto)}
        
        PREGUNTA: {pregunta}
        
        Responde usando el contexto. Si hay c√≥digo, √∫salo. Formato JSON.
        {{ "respuesta": "...", "codigo": "..." }}
        """
        res = modelo.generate_content(prompt, generation_config={"response_mime_type": "application/json"})
        return jsonify(json.loads(limpiar_json(res.text)))
        
    else:
        print("Empty - ü§∑ No s√© la respuesta. Iniciando protocolo de APRENDIZAJE...")
        # 4. APRENDIZAJE (Si falla la memoria interna)
        respuesta_aprendida = aprender_y_guardar(pregunta)
        
        return jsonify({
            "respuesta": respuesta_aprendida,
            "fuente": "Investigaci√≥n en Tiempo Real (Nuevo Conocimiento)",
            "estado": "Aprendido y Guardado"
        })

if __name__ == "__main__":
    app.run(debug=True, port=5000)
